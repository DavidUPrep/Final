{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## David Abramowitz - Big Data Final\n",
    "Tuesday, December 18th "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "In this lab I attempted to use predictive models to hopefully estimate what students would have scored on the third test. In every model that I ran, the accuracy score was so low that estimating scores seemed pointless.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Prep\n",
    "This project looked at grading data from 12th Grade Physics taught by Moses Rifkin. The data covered academic years 2016-17 to 2018-19. For the first two academic years students took four tests and two finals, with the fourth test being in between the first and second final exams. For each test and final exam, students had the oppurtunity to estimate their score and rate their feelings regarding the test. The feelings score is out of ten. This data was collected alongside each test/final. The notable exception is in the 2018-19 data: data only exists for the first two tests (the third test was canceled). Throughout all three years, blank cells exist in every category. For all of the data, sex (M/F) was collected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "#Viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#machine learning models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('Physics_train3.csv')\n",
    "test_df = pd.read_csv ('Physics_test3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import numpy\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "In Google Sheets I did the following:\n",
    "- Deleted the test headers and renamed the columns\n",
    "- Combined 2016/17 and 2017/18 data to create an effective amount of training data for 2018/19\n",
    "- Removed score estimations and confidence ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below, I used code from Jason Brownlee which he posted on his GitHub page to remove rows containing one or more NaNs. https://machinelearningmastery.com/handle-missing-data-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Becuase the data had already been cleaned, there was not much of note in the exploration phase. Even after the rows with null shave been removed, simple division indicates that each class has roughly the same number of students in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>T1_Score</th>\n",
       "      <th>T2_Score</th>\n",
       "      <th>T3_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>69.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>94.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>46.0</td>\n",
       "      <td>57.5</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>61.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>96.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F</td>\n",
       "      <td>77.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "      <td>79.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M</td>\n",
       "      <td>85.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F</td>\n",
       "      <td>71.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M</td>\n",
       "      <td>84.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>90.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Gender  T1_Score  T2_Score  T3_Score\n",
       "0      F      69.0      89.0      77.0\n",
       "1      M      94.0      90.0      93.0\n",
       "2      F      46.0      57.5      62.0\n",
       "3      M      61.0      94.0      79.0\n",
       "4      M      96.0      97.0      62.0\n",
       "5      F      77.0      73.0      79.0\n",
       "6      M      79.0      95.0      86.0\n",
       "7      M      85.0      88.0      75.0\n",
       "8      F      71.0      83.0      73.0\n",
       "9      M      84.0      99.0      90.5"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing first 10 rows to ensure accuracy\n",
    "train_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gender' 'T1_Score' 'T2_Score' 'T3_Score']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 82 entries, 0 to 84\n",
      "Data columns (total 4 columns):\n",
      "Gender      82 non-null object\n",
      "T1_Score    82 non-null float64\n",
      "T2_Score    82 non-null float64\n",
      "T3_Score    82 non-null float64\n",
      "dtypes: float64(3), object(1)\n",
      "memory usage: 3.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#printing the columns\n",
    "print(train_df.columns.values)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gender' 'T1_Score' 'T2_Score']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 23 entries, 0 to 22\n",
      "Data columns (total 3 columns):\n",
      "Gender      23 non-null object\n",
      "T1_Score    23 non-null int64\n",
      "T2_Score    23 non-null float64\n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 736.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "print(test_df.columns.values)\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My feature manipulation consisted of replacing the sex values with numerical values. I also Dropped every row that contained a NaN, however this may qualify as data cleaning rather than feature manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing sex values with numerical ones\n",
    "train_df = train_df.replace({'M': '2', 'F': '1'})\n",
    "test_df = test_df.replace({'M': '2', 'F': '1'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code to replace \"M' and 'F' came from Andy Hayden on StackOverflow (https://stackoverflow.com/questions/18548662/rename-elements-in-a-column-of-a-data-frame-using-pandas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NO FEATURE ELIMINATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Becuase I had previously removed the estimated and confidence scores in Google Sheets (I thought that they may be altering the accuracy scores of the models), there were no features to eliminate here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting train data into a training and a testing set\n",
    "train_labels = train_df['T3_Score']\n",
    "train_features = train_df.drop(['T3_Score'], axis=1)\n",
    "#valid_train_features = train_df['T3_Score']\n",
    "\n",
    "test_features = test_df\n",
    "\n",
    "\n",
    "#train_labels = train_df.drop(['T1_Pred'],['T1_Feel'] axis=1)\n",
    "#features - x\n",
    "#labels - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_features:  (82, 3)  shape of train_labels:  (82,) Shape of test_features:  (23, 3)\n",
      "train_features feature list:  ['Gender' 'T1_Score' 'T2_Score']\n",
      "test_features feature list:  ['Gender' 'T1_Score' 'T2_Score']\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of train_features: \", train_features.shape, \" shape of train_labels: \", train_labels.shape, \"Shape of test_features: \", test_features.shape)\n",
    "print(\"train_features feature list: \", train_features.columns.values)\n",
    "print(\"test_features feature list: \", test_features.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the shapes of the various lists were examined. Then, the training data was split so that cross validation could be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16, 3), (66, 3), (66,), (66,))"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SPlit traning data in order to cross validate\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "valid_train_features, valid_test_features, valid_train_labels, valid_test_labels  = train_test_split(train_features, train_labels, train_size=0.2, test_size=0.8)\n",
    "\n",
    "valid_train_features.shape, valid_test_features.shape, valid_test_labels.shape, valid_test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Gender  T1_Score  T2_Score\n",
      "84      1      75.0      73.0\n",
      "2       1      46.0      57.5\n",
      "79      1      71.0      87.0\n",
      "59      2      53.0      64.0\n",
      "15      2      90.0     100.0\n",
      "61      1      50.0      59.0\n",
      "6       2      79.0      95.0\n",
      "81      2      46.0      55.0\n",
      "82      1      95.0      91.0\n",
      "56      2      36.0      80.0\n",
      "51      1      58.0      89.0\n",
      "45      1      56.0      69.5\n",
      "14      1      76.0      76.5\n",
      "21      2      90.0     100.0\n",
      "5       1      77.0      73.0\n",
      "3       2      61.0      94.0\n"
     ]
    }
   ],
   "source": [
    "print(valid_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn                        import metrics, svm\n",
    "from sklearn.linear_model           import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_enc = preprocessing.LabelEncoder()\n",
    "valid_train_labels = lab_enc.fit_transform(valid_train_labels)\n",
    "valid_test_labels = lab_enc.fit_transform(valid_test_labels)\n",
    "train_labels = lab_enc.fit_transform(train_labels)\n",
    "\n",
    "#print(training_scores_encoded)\n",
    "#print(utils.multiclass.type_of_target(training_scores_Y))\n",
    "#print(utils.multiclass.type_of_target(training_scores_Y.astype('int')))\n",
    "#print(utils.multiclass.type_of_target(training_scores_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Gender  T1_Score  T2_Score\n",
      "84      1      75.0      73.0\n",
      "2       1      46.0      57.5\n",
      "79      1      71.0      87.0\n",
      "59      2      53.0      64.0\n",
      "15      2      90.0     100.0\n",
      "61      1      50.0      59.0\n",
      "6       2      79.0      95.0\n",
      "81      2      46.0      55.0\n",
      "82      1      95.0      91.0\n",
      "56      2      36.0      80.0\n",
      "51      1      58.0      89.0\n",
      "45      1      56.0      69.5\n",
      "14      1      76.0      76.5\n",
      "21      2      90.0     100.0\n",
      "5       1      77.0      73.0\n",
      "3       2      61.0      94.0\n"
     ]
    }
   ],
   "source": [
    "print(valid_train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data models\n",
    "I chose the following models primarily becuase I was comfortable with them. I had attempted to use other models, however I encountered problems with producing an accuracy score. becuase of how low these scores are, I suspect that no model would be able to achieve a significant accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030303030303030304\n",
      "0.012195121951219513 0.4375\n"
     ]
    }
   ],
   "source": [
    "#log reg\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(valid_train_features, valid_train_labels)\n",
    "log_pred = logreg.predict(valid_test_features)\n",
    "print(accuracy_score(valid_test_labels, log_pred))\n",
    "\n",
    "#Compare the accuracy of running the whole set vs the cross validated set so as to avoid overfitting\n",
    "print (logreg.score( train_features , train_labels ) , logreg.score( valid_train_features , valid_train_labels ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.045454545454545456\n",
      "0.024390243902439025 0.9375\n"
     ]
    }
   ],
   "source": [
    "#RandomForest\n",
    "RFC = RandomForestClassifier()\n",
    "RFC.fit(valid_train_features, valid_train_labels)\n",
    "RFC_pred = RFC.predict(valid_test_features)\n",
    "accuracy_score_RFC = (accuracy_score(valid_test_labels, RFC_pred))\n",
    "print(accuracy_score_RFC)\n",
    "\n",
    "#Compare the accuracy of running the whole set vs the cross validated set so as to avoid overfitting\n",
    "print (RFC.score( train_features , train_labels ) , RFC.score( valid_train_features , valid_train_labels ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015151515151515152\n",
      "0.012195121951219513 0.25\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(valid_train_features, valid_train_labels)\n",
    "KNN_Pred = knn_clf.predict(valid_test_features)\n",
    "print(accuracy_score(valid_test_labels, KNN_Pred))\n",
    "\n",
    "#Compare the accuracy of running the whole set vs the cross validated set so as to avoid overfitting\n",
    "print (knn_clf.score( train_features , train_labels ) , knn_clf.score( valid_train_features , valid_train_labels ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.024390243902439025 0.875\n"
     ]
    }
   ],
   "source": [
    "#GaussianNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "GNB = GaussianNB()\n",
    "\n",
    "GNB.fit(valid_train_features, valid_train_labels)\n",
    "GNB_pred = GNB.predict(valid_test_features)\n",
    "print(accuracy_score(valid_test_labels, GNB_pred))\n",
    "print (GNB.score( train_features , train_labels ) , GNB.score( valid_train_features , valid_train_labels ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that these accuracy scores are so low (surprisingly low), it seems pointless to produce as estimate of individual scores. An area for future exploration would be why these scores are so low. I am somewhat confident in my work, but it is odd that no model would be able to produce even a moderatly high accuracy score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis and conclusion\n",
    "One major area for future exploration would how sex factors into grade trends. It should be noted that Moses Rifkin grades tests and finals blindly; that is, tests are turned in with a number instead of a name. As such, Mr. Rifkin is unaware of students' sex when grading their tests. It would also be interesting to examine why the accuracy scores are so low. It is not uncommon for there to be great volatility between the first and second tests, and the data does not include or account for any changes in teaching style or course load. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aknowledgements\n",
    "All code except for the code to remove rows with NaNs, which came from Jason Brownlee on GitHub (https://machinelearningmastery.com/handle-missing-data-python/), and the code to replace M and F (Andy hayden - StackOverflow (https://stackoverflow.com/questions/18548662/rename-elements-in-a-column-of-a-data-frame-using-pandas)) came from Ms. Sconyers. A big thank you to Moses Rifkin, who volunteered his grading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
